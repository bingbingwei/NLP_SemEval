{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 2: Semantic Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "from pyquery import PyQuery as pq\n",
    "import re\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import nltk\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser, CoreNLPParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/'\n",
    "train_file = 'TRAIN_FILE.txt'\n",
    "test_file = 'TEST_FILE.txt'\n",
    "test_ans_file = 'answer_key.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = ['Cause-Effect(e1,e2)', 'Cause-Effect(e2,e1)', 'Instrument-Agency(e1,e2)', 'Instrument-Agency(e2,e1)', \n",
    "         'Product-Producer(e1,e2)', 'Product-Producer(e2,e1)', 'Content-Container(e1,e2)', 'Content-Container(e2,e1)', \n",
    "         'Entity-Origin(e1,e2)', 'Entity-Origin(e2,e1)', 'Entity-Destination(e1,e2)', 'Entity-Destination(e2,e1)',\n",
    "         'Component-Whole(e1,e2)', 'Component-Whole(e2,e1)', 'Member-Collection(e1,e2)', 'Member-Collection(e2,e1)',\n",
    "         'Message-Topic(e1,e2)', 'Message-Topic(e2,e1)', 'Other']\n",
    "\n",
    "def parse_sentence(line):\n",
    "    index, sentence_raw = line.split('\\t')\n",
    "    entities = [pq(sentence_raw)('e%d' % i).text() for i in [1, 2]]\n",
    "    sentence = re.sub(r'</?e.>|[\"\\n]', '', sentence_raw)\n",
    "    return index, sentence, entities\n",
    "\n",
    "def parse_case(line):\n",
    "    if line[-1] == '\\n': line = line[:-1]\n",
    "    if line in cases:\n",
    "        return cases.index(line)\n",
    "    else:\n",
    "        return -1\n",
    "        \n",
    "def read_dataset(data_dir, train_file, test_file, test_ans_file):\n",
    "    train_data, test_data = [], []\n",
    "    \n",
    "    with open(data_dir + train_file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if '\\t' in line:\n",
    "                index, sent, ent = parse_sentence(line)\n",
    "            else:\n",
    "                case = parse_case(line)\n",
    "                if case >= 0:\n",
    "                    train_data += [{'index': int(index), 'sentence': sent, 'entity': ent, 'label': case}]\n",
    "                    \n",
    "        train_df = pd.DataFrame(train_data).set_index('index')\n",
    "        \n",
    "    with open(data_dir + test_file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if '\\t' in line:\n",
    "                index, sent, ent = parse_sentence(line)\n",
    "                test_data += [{'index': int(index), 'sentence': sent, 'entity': ent, 'label': -1}]\n",
    "                    \n",
    "        test_df = pd.DataFrame(test_data).set_index('index')\n",
    "        \n",
    "    with open(data_dir + test_ans_file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if '\\t' in line:\n",
    "                index, answer = line.split('\\t')\n",
    "                test_df.at[int(index), 'label'] = parse_case(answer)\n",
    "        \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df, test_df = read_dataset(data_dir, train_file, test_file, test_ans_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Using `nltk.parse.corenlp.CoreNLPDependencyParser` **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.6 ms, sys: 13.3 ms, total: 33.8 ms\n",
      "Wall time: 728 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<list_iterator at 0x124d47dd8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdp = CoreNLPDependencyParser()\n",
    "%time cdp.parse(sentence.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Using `nltk.parse.stanford.StanfordDependencyParser` **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.StanforCoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.75 ms, sys: 25.3 ms, total: 34.1 ms\n",
      "Wall time: 3.31 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<list_iterator at 0x1253c5198>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['CLASSPATH'] = '../stanford-parser-full-2018-02-27'\n",
    "sdp = StanfordDependencyParser()\n",
    "%time sdp.parse(sentence.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Find the dependency path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Dependency tree **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_dependency_tree(sentence):\n",
    "    tree, = cdp.parse(sentence.split())\n",
    "    tree_list = [('', 'ROOT', '0', 'ROOT')] + [tuple(r.split('\\t')) for r in tree.to_conll(4).split('\\n')][:-1]\n",
    "    return tree_list, tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Dependency path **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_entity_index(tree_list, e):\n",
    "    for i, d in enumerate(tree_list):\n",
    "        if d[0] == e: return i\n",
    "    return -1\n",
    "\n",
    "def find_root_path(tree_list, e):\n",
    "    path = []\n",
    "    i = find_entity_index(tree_list, e)\n",
    "    while True:\n",
    "        ent, tag, parent, arrow = tree_list[i]\n",
    "        path += [[ent, tag, arrow]]\n",
    "        i = int(parent)\n",
    "        if arrow is 'ROOT': break\n",
    "    return path\n",
    "\n",
    "def _merge_path(p1, p2):\n",
    "    rp1, rp2 = p1[::-1], p2[::-1]\n",
    "    max_len = min(len(p1), len(p2))\n",
    "    \n",
    "    path = []\n",
    "    for i in range(max_len):\n",
    "        if rp1[i] == rp2[i]:\n",
    "            m1 = len(p1) - i - 1\n",
    "            m2 = i\n",
    "        else:\n",
    "            m1 = len(p1) - i\n",
    "            m2 = i - 1\n",
    "            break\n",
    "    \n",
    "    path = p1[:m1] + rp2[m2:]\n",
    "    return path, m1\n",
    "\n",
    "def fix_transition_and_direction(path, mp):\n",
    "    path[mp][2] = 'end'\n",
    "    for i in range(mp, len(path)-1): # shift forward the transition tag from merge point\n",
    "        path[i][2] = path[i+1][2]\n",
    "        path[i+1][2] = 'end'\n",
    "        \n",
    "    for i in range(len(path)): # before merge point: 1, after merge point: 0\n",
    "        path[i][2] = path[i][2].split(':')[0]\n",
    "        if i < mp: path[i][2] += '-inv'\n",
    "            \n",
    "    return path\n",
    "\n",
    "def merge_path(p1, p2):\n",
    "    path, mp = _merge_path(p1, p2)\n",
    "    \n",
    "    if len(path) == 0 or mp < 0:\n",
    "        print(\"Can't merge two path\")\n",
    "        \n",
    "    else:\n",
    "        path = fix_transition_and_direction(path, mp)\n",
    "                \n",
    "    return path\n",
    "\n",
    "def find_dependency_path(tree_list, entities):\n",
    "    e1, e2 = entities\n",
    "    p1 = find_root_path(tree_list, e1)\n",
    "    p2 = find_root_path(tree_list, e2)\n",
    "    path = merge_path(p1, p2)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The brown fox is quick and he is jumping over the lazy dog\n",
      "[['The', 'DT', 'det-inv'], ['fox', 'NN', 'nsubj-inv'], ['quick', 'JJ', 'conj'], ['jumping', 'VBG', 'dobj'], ['dog', 'NN', 'end']]\n"
     ]
    }
   ],
   "source": [
    "print(sentence)\n",
    "tree_list, tree = find_dependency_tree(sentence)\n",
    "path = find_dependency_path(tree_list, ['The', 'dog'])\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"522pt\" height=\"388pt\"\n",
       " viewBox=\"0.00 0.00 521.87 388.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 384)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-384 517.8657,-384 517.8657,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"182.7949\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"182.7949\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">5 (quick)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;5 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M182.7949,-343.7616C182.7949,-332.3597 182.7949,-317.4342 182.7949,-304.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"186.295,-304.2121 182.7949,-294.2121 179.295,-304.2121 186.295,-304.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"201.8501\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ROOT</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"74.7949\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3 (fox)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;3 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>5&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M153.6904,-257.9883C145.3435,-252.4862 136.3662,-246.2353 128.457,-240 118.8666,-232.4392 108.9304,-223.526 100.2536,-215.3348\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"102.634,-212.7682 92.9931,-208.3744 97.7898,-217.8213 102.634,-212.7682\"/>\n",
       "<text text-anchor=\"middle\" x=\"143.9639\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"146.7949\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4 (is)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;4 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>5&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M175.1602,-257.7616C170.2919,-246.1316 163.889,-230.8357 158.3986,-217.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"161.5086,-216.085 154.4186,-208.2121 155.0515,-218.788 161.5086,-216.085\"/>\n",
       "<text text-anchor=\"middle\" x=\"177.9019\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cop</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"219.7949\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">6 (and)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M190.6417,-257.7616C195.6453,-246.1316 202.2261,-230.8357 207.8689,-217.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"211.2224,-218.7813 211.9595,-208.2121 204.7923,-216.0148 211.2224,-218.7813\"/>\n",
       "<text text-anchor=\"middle\" x=\"210.0088\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cc</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"306.7949\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">9 (jumping)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;9 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>5&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M208.7894,-257.9716C227.3071,-245.1286 252.3826,-227.7376 272.5335,-213.7619\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"274.6122,-216.5797 280.8347,-208.0047 270.6229,-210.8277 274.6122,-216.5797\"/>\n",
       "<text text-anchor=\"middle\" x=\"264.8467\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">conj</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"28.7949\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 (The)</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"111.7949\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2 (brown)</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>3&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M65.0395,-171.7616C58.7578,-160.0176 50.4767,-144.5355 43.4159,-131.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"46.3391,-129.3792 38.5363,-122.2121 40.1666,-132.6808 46.3391,-129.3792\"/>\n",
       "<text text-anchor=\"middle\" x=\"63.3467\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>3&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M82.6417,-171.7616C87.6453,-160.1316 94.2261,-144.8357 99.8689,-131.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"103.2224,-132.7813 103.9595,-122.2121 96.7923,-130.0148 103.2224,-132.7813\"/>\n",
       "<text text-anchor=\"middle\" x=\"111.3467\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">amod</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"201.7949\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">7 (he)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;7 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>9&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M278.7718,-171.917C270.7323,-166.4133 262.083,-160.1794 254.457,-154 245.1578,-146.4647 235.5132,-137.6419 227.0583,-129.5228\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"229.2756,-126.7964 219.6721,-122.3215 224.3889,-131.8084 229.2756,-126.7964\"/>\n",
       "<text text-anchor=\"middle\" x=\"269.9639\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"273.7949\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">8 (is)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;8 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>9&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M299.7965,-171.7616C295.3338,-160.1316 289.4645,-144.8357 284.4317,-131.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"287.6336,-130.2945 280.7833,-122.2121 281.0982,-132.8023 287.6336,-130.2945\"/>\n",
       "<text text-anchor=\"middle\" x=\"302.9019\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">aux</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"352.7949\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">10 (over)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M316.5503,-171.7616C322.832,-160.0176 331.1132,-144.5355 338.1739,-131.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"341.4232,-132.6808 343.0536,-122.2121 335.2507,-129.3792 341.4232,-132.6808\"/>\n",
       "<text text-anchor=\"middle\" x=\"372.0674\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound:prt</text>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>13</title>\n",
       "<text text-anchor=\"middle\" x=\"440.7949\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">13 (dog)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;13 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>9&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M348.1439,-184.2276C370.3124,-179.3326 396.8778,-170.3612 415.7949,-154 422.6537,-148.0679 427.8362,-139.8052 431.6523,-131.7035\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"434.9067,-132.9929 435.523,-122.416 428.4454,-130.3 434.9067,-132.9929\"/>\n",
       "<text text-anchor=\"middle\" x=\"439.2397\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dobj</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>11</title>\n",
       "<text text-anchor=\"middle\" x=\"399.7949\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">11 (the)</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;11 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>13&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M432.0999,-85.7616C426.501,-74.0176 419.12,-58.5355 412.8267,-45.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"415.9402,-43.7326 408.4774,-36.2121 409.6216,-46.745 415.9402,-43.7326\"/>\n",
       "<text text-anchor=\"middle\" x=\"431.3467\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"480.7949\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">12 (lazy)</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;12 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>13&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M449.2779,-85.7616C454.7402,-74.0176 461.9412,-58.5355 468.081,-45.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"471.2803,-46.7554 472.3242,-36.2121 464.9333,-43.8032 471.2803,-46.7554\"/>\n",
       "<text text-anchor=\"middle\" x=\"478.3467\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">amod</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<DependencyGraph with 14 nodes>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Preprocess Data: Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Word tokenizer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_tokenizer(train_df, test_df):\n",
    "    word_tokenizer = Tokenizer()\n",
    "    all_corpus = list(train_df.loc[:]['sentence']) + list(test_df.loc[:]['sentence'])\n",
    "    word_tokenizer.fit_on_texts(all_corpus)\n",
    "    return word_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = get_word_tokenizer(train_df, test_df)\n",
    "word_index = word_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** POS tags & dependency tags **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tags():\n",
    "    pos_tags = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP',\n",
    "                'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', \n",
    "                'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
    "\n",
    "    dep_half_tags = ['nsubj', 'nsubjpass', 'dobj', 'iobj', 'csubj', 'csubjpass', 'ccomp', 'xcomp',\n",
    "                     'nmod', 'advcl', 'advmod', 'neg',\n",
    "                     'vocative', 'discourse', 'expl', 'aux', 'auxpass', 'cop', 'mark', 'punct',\n",
    "                     'nummod', 'appos', 'acl', 'amod', 'det',\n",
    "                     'compound', 'name', 'mwe', 'foreign', 'goeswith', \n",
    "                     'conj', 'cc',\n",
    "                     'case', 'list', 'dislocated', 'parataxis', 'remnant', 'reparandum',\n",
    "                     'root', 'dep']\n",
    "    dep_tags = []\n",
    "    for tag in dep_half_tags:\n",
    "        dep_tags += [tag]\n",
    "        dep_tags += [tag + '-inv']\n",
    "\n",
    "    dep_tags = list(np.sort(dep_tags)) + ['end']\n",
    "    \n",
    "    return pos_tags, dep_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Convert POS tags & dependency tags to sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_path_content(path, pos_tags, dep_tags):\n",
    "    path_sent, pos_seq, dep_seq = '', [], []\n",
    "    \n",
    "    for node in path:\n",
    "        word, pt, dt = node\n",
    "        path_sent += word + ' '\n",
    "        if pt not in pos_tags: pos_tags += [pt]\n",
    "        if dt not in dep_tags: dep_tags += [dt]\n",
    "        pos_seq += [pos_tags.index(pt)]\n",
    "        dep_seq += [dep_tags.index(dt)]\n",
    "    \n",
    "    return path_sent, pos_seq, dep_seq, pos_tags, dep_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fox quick jumping dog  \n",
      " [2, 11, 6, 28, 11] \n",
      " [33, 61, 22, 38, 80]\n"
     ]
    }
   ],
   "source": [
    "pos_tags, dep_tags = init_tags()\n",
    "sent, pos, dep, pos_tags, dep_tags = split_path_content(path, pos_tags, dep_tags)\n",
    "print(sent, '\\n', pos, '\\n', dep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Preprocess Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_data(df, word_tokenizer, pos_tags, dep_tags, max_seq_len):\n",
    "    data, path_sent, pos_seq, dep_seq, labels = {}, [], [], [], []\n",
    "    \n",
    "    if debug:\n",
    "        f = open('log.txt', 'wa')\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        tree_list, tree = find_dependency_tree(row['sentence'])\n",
    "        path = find_dependency_path(tree_list, row['entity'])\n",
    "        print('\\r[%d]' % i, end='')\n",
    "        if debug:\n",
    "            print('[%d]' % i, file=f)\n",
    "            print(row['sentence'], row['entity'], file=f)\n",
    "            print(tree_list, file=f)\n",
    "            print(path, '\\n', file=f)\n",
    "        psent, pos, dep, pos_tags, dep_tags = split_path_content(path, pos_tags, dep_tags)\n",
    "        \n",
    "        path_sent += [psent]\n",
    "        pos_seq += [pos]\n",
    "        dep_seq += [dep]\n",
    "        labels += [row['label']]\n",
    "    \n",
    "    if debug:\n",
    "        f.close()\n",
    "            \n",
    "    path_seq = word_tokenizer.texts_to_sequences(path_sent)\n",
    "    data['path_seq'] = pad_sequences(path_seq, maxlen=max_seq_len)\n",
    "    data['pos_seq'] = pad_sequences(pos_seq, maxlen=max_seq_len)\n",
    "    data['dep_seq'] = pad_sequences(dep_seq, maxlen=max_seq_len)\n",
    "    data['labels'] = to_categorical(labels)\n",
    "    \n",
    "    return data, pos_tags, dep_tags\n",
    "\n",
    "def preprocess_data(train_df, test_df, max_seq_len=8):\n",
    "    word_tokenizer = get_word_tokenizer(train_df, test_df)\n",
    "    \n",
    "    pos_tags, dep_tags = init_tags()\n",
    "    \n",
    "    train_data, pos_tags, dep_tags = \\\n",
    "        _preprocess_data(train_df, word_tokenizer, pos_tags, dep_tags, max_seq_len)\n",
    "    test_data, pos_tags, dep_tags = \\\n",
    "        _preprocess_data(test_df, word_tokenizer, pos_tags, dep_tags, max_seq_len)\n",
    "    \n",
    "    return train_data, test_data, pos_tags, dep_tags, word_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10717]CPU times: user 46.9 s, sys: 5.66 s, total: 52.5 s\n",
      "Wall time: 4min 23s\n"
     ]
    }
   ],
   "source": [
    "debug = False\n",
    "%time train_data, test_data, pos_tags, dep_tags, word_tokenizer = preprocess_data(train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Embedding Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Read GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove(data_dir, glove_file):\n",
    "    word_vectors = []\n",
    "    with open(data_dir + glove_file, 'r', encoding='UTF-8') as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            word_vectors += [{'word': word, 'vector': list(map(float, vector))}]\n",
    "\n",
    "    return pd.DataFrame(word_vectors).set_index('word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.8 s, sys: 19.7 s, total: 1min 15s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "glove_file = \"glove.6B/glove.6B.300d.txt\"\n",
    "%time word_vectors = read_glove(data_dir, glove_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Construct Embedding Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding_layer(word_index, word_vectors, max_sequence_len=8):\n",
    "    embedding_dim = 300\n",
    "    word_len = len(word_index) + 1\n",
    "    word_embedding_matrix = np.zeros((word_len, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        word_vector = word_vectors.get(word)\n",
    "        if word_vector is not None:\n",
    "            word_embedding_matrix[i] = word_vector\n",
    "    \n",
    "    word_embedding_layer = Embedding(word_len, embedding_dim, weights=[word_embedding_matrix],\n",
    "                                     input_length=max_sequence_len, trainable=False)\n",
    "    \n",
    "    return word_embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag_embedding_layer(tag_len, max_sequence_len=8):\n",
    "    tag_embedding_list = []\n",
    "    for i in range(tag_len):\n",
    "        vector = [0] * tag_len\n",
    "        vector[i] = 1\n",
    "        tag_embedding_list += [vector]\n",
    "        \n",
    "    tag_embedding_matrix = np.array(tag_embedding_list)\n",
    "    tag_embedding_layer = Embedding(tag_len, tag_len, weights=[tag_embedding_matrix],\n",
    "                                    input_length=max_sequence_len, trainable=False)\n",
    "    \n",
    "    return tag_embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Construct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Dropout\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_model(max_sequence_len, word_dim, pos_dim, dep_dim, categories,\n",
    "                    word_index, word_vectors):\n",
    "    \n",
    "    in_word = Input(shape=(max_sequence_len,), name='Input-Word')\n",
    "    in_pos = Input(shape=(max_sequence_len,), name='Input-POS')\n",
    "    in_dep = Input(shape=(max_sequence_len,), name='Input-Dependency')\n",
    "    \n",
    "    emb_word = get_word_embedding_layer(word_index, word_vectors, max_sequence_len)(in_word)\n",
    "    emb_pos = get_tag_embedding_layer(pos_dim, max_sequence_len)(in_pos)\n",
    "    emb_dep = get_tag_embedding_layer(dep_dim, max_sequence_len)(in_dep)\n",
    "    \n",
    "    lstm_word = LSTM(128, activation='relu', dropout=0.2, name='LSTM-Word')(emb_word)\n",
    "    lstm_pos = LSTM(32, activation='relu', dropout=0.2, name='LSTM-POS')(emb_pos)\n",
    "    lstm_dep = LSTM(32, activation='relu', dropout=0.2, name='LSTM-Dependency')(emb_dep)\n",
    "    \n",
    "    x = Concatenate(name='Concatenate')([lstm_word, lstm_pos, lstm_dep])\n",
    "    x = Dense(64, activation='relu', name='Dense')(x)\n",
    "    x = Dropout(0.2, name='Dropout')(x)\n",
    "    out = Dense(categories, activation='softmax', name='Output')(x)\n",
    "    \n",
    "    model = Model(inputs=[in_word, in_pos, in_dep], outputs=out)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = 8\n",
    "word_dim = 300\n",
    "pos_dim = len(pos_tags)\n",
    "dep_dim = len(dep_tags)\n",
    "categories = 19\n",
    "word_index = word_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "Input-Word (InputLayer)          (None, 8)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "Input-POS (InputLayer)           (None, 8)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "Input-Dependency (InputLayer)    (None, 8)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)          (None, 8, 300)        6891000     Input-Word[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)          (None, 8, 40)         1600        Input-POS[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)          (None, 8, 81)         6561        Input-Dependency[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "LSTM-Word (LSTM)                 (None, 128)           219648      embedding_4[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "LSTM-POS (LSTM)                  (None, 32)            9344        embedding_5[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "LSTM-Dependency (LSTM)           (None, 32)            14592       embedding_6[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "Concatenate (Concatenate)        (None, 192)           0           LSTM-Word[0][0]                  \n",
      "                                                                   LSTM-POS[0][0]                   \n",
      "                                                                   LSTM-Dependency[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "Dense (Dense)                    (None, 64)            12352       Concatenate[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "Dropout (Dropout)                (None, 64)            0           Dense[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "Output (Dense)                   (None, 19)            1235        Dropout[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 7,156,332\n",
      "Trainable params: 257,171\n",
      "Non-trainable params: 6,899,161\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = construct_model(max_sequence_len, word_dim, pos_dim, dep_dim, categories, word_index, word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, model_path, train_data, test_data):\n",
    "    \n",
    "    train_X = [train_data[k] for k in ['path_seq', 'pos_seq', 'dep_seq']]\n",
    "    train_Y = train_data['labels']\n",
    "    \n",
    "    test_X = [test_data[k] for k in ['path_seq', 'pos_seq', 'dep_seq']]\n",
    "    test_Y = test_data['labels']\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min')\n",
    "    cp = ModelCheckpoint(monitor='val_loss', save_best_only=True, save_weights_only=False,\n",
    "                         mode='min', filepath=model_path)\n",
    "    \n",
    "    history = model.fit(train_X, train_Y, validation_data=(test_X, test_Y),\n",
    "                         epochs=30, verbose=1, batch_size=32, callbacks=[es, cp])\n",
    "\n",
    "    hist = history.history\n",
    "    return model, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2717 samples\n",
      "Epoch 1/30\n",
      "8000/8000 [==============================] - 52s - loss: 0.2555 - val_loss: 0.1923\n",
      "Epoch 2/30\n",
      "8000/8000 [==============================] - 26s - loss: 0.1925 - val_loss: 0.1849\n",
      "Epoch 3/30\n",
      "8000/8000 [==============================] - 26s - loss: 0.1827 - val_loss: 0.1726\n",
      "Epoch 4/30\n",
      "8000/8000 [==============================] - 25s - loss: 0.1752 - val_loss: 0.1696\n",
      "Epoch 5/30\n",
      "8000/8000 [==============================] - 25s - loss: 0.1708 - val_loss: 0.1634\n",
      "Epoch 6/30\n",
      "8000/8000 [==============================] - 26s - loss: 0.1678 - val_loss: 0.1607\n",
      "Epoch 7/30\n",
      "8000/8000 [==============================] - 25s - loss: 0.1659 - val_loss: 0.1601\n",
      "Epoch 8/30\n",
      "8000/8000 [==============================] - 25s - loss: 0.1632 - val_loss: 0.1568\n",
      "Epoch 9/30\n",
      "8000/8000 [==============================] - 25s - loss: 0.1624 - val_loss: 0.1577\n",
      "Epoch 10/30\n",
      "8000/8000 [==============================] - 26s - loss: 0.1606 - val_loss: 0.1555\n",
      "Epoch 11/30\n",
      "8000/8000 [==============================] - 26s - loss: 0.1595 - val_loss: 0.1553\n",
      "Epoch 12/30\n",
      "8000/8000 [==============================] - 26s - loss: 0.1592 - val_loss: 0.1544\n",
      "Epoch 13/30\n",
      "8000/8000 [==============================] - 28s - loss: 0.1584 - val_loss: 0.1545\n",
      "Epoch 14/30\n",
      "8000/8000 [==============================] - 26s - loss: 0.1576 - val_loss: 0.1535\n",
      "Epoch 15/30\n",
      "8000/8000 [==============================] - 26s - loss: 0.1572 - val_loss: 0.1517\n",
      "Epoch 16/30\n",
      "8000/8000 [==============================] - 25s - loss: 0.1563 - val_loss: 0.1511\n",
      "Epoch 17/30\n",
      "8000/8000 [==============================] - 27s - loss: 0.1560 - val_loss: 0.1502\n",
      "Epoch 18/30\n",
      "8000/8000 [==============================] - 26s - loss: 0.1558 - val_loss: 0.1500\n",
      "Epoch 19/30\n",
      "8000/8000 [==============================] - 26s - loss: 0.1551 - val_loss: 0.1514\n",
      "Epoch 20/30\n",
      "8000/8000 [==============================] - 26s - loss: 0.1545 - val_loss: 0.1502\n",
      "Epoch 21/30\n",
      "8000/8000 [==============================] - 27s - loss: 0.1544 - val_loss: 0.1499\n",
      "Epoch 22/30\n",
      "8000/8000 [==============================] - 26s - loss: 0.1535 - val_loss: 0.1493\n",
      "Epoch 23/30\n",
      "8000/8000 [==============================] - 27s - loss: 0.1539 - val_loss: 0.1501\n",
      "Epoch 24/30\n",
      "8000/8000 [==============================] - 33s - loss: 0.1533 - val_loss: 0.1490\n",
      "Epoch 25/30\n",
      "8000/8000 [==============================] - 25s - loss: 0.1530 - val_loss: 0.1488\n",
      "Epoch 26/30\n",
      "8000/8000 [==============================] - 22s - loss: 0.1524 - val_loss: 0.1489\n",
      "Epoch 27/30\n",
      "8000/8000 [==============================] - 22s - loss: 0.1527 - val_loss: 0.1483\n",
      "Epoch 28/30\n",
      "8000/8000 [==============================] - 25s - loss: 0.1521 - val_loss: 0.1476\n",
      "Epoch 29/30\n",
      "8000/8000 [==============================] - 21s - loss: 0.1516 - val_loss: 0.1487\n",
      "Epoch 30/30\n",
      "8000/8000 [==============================] - 24s - loss: 0.1514 - val_loss: 0.1474\n"
     ]
    }
   ],
   "source": [
    "model, hist = train_model(model, '../model/lstm.h5', train_data, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
